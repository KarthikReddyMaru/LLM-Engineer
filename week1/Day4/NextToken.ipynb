{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "692fbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_MODEL = 'gpt-oss:latest'\n",
    "OLLAMA_BASE_URL = 'http://localhost:11434/v1'\n",
    "OLLAMA_API_KEY = \"CAN_BE_ANYTHING\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = OLLAMA_BASE_URL,\n",
    "    api_key = OLLAMA_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a28a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\", \"content\": \"Tell me a joke\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1380e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next_Token: Why\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token: Why, Prob: -0.0265766903758049\n",
      "Token: Sure, Prob: -3.8055531978607178\n",
      "Token: What, Prob: -6.040942668914795\n",
      "Token: Here, Prob: -7.013219356536865\n",
      "Token: **, Prob: -7.701449871063232\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  don\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  don, Prob: -0.37730905413627625\n",
      "Token:  did, Prob: -1.5261127948760986\n",
      "Token:  don't, Prob: -2.409306764602661\n",
      "Token:  was, Prob: -5.73049259185791\n",
      "Token:  do, Prob: -6.161084175109863\n",
      "\n",
      "\n",
      "\n",
      "Next_Token: ’t\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token: ’t, Prob: -6.382289257089724e-07\n",
      "Token:  , Prob: -15.193207740783691\n",
      "Token: ‘t, Prob: -15.248995780944824\n",
      "Token: ‑, Prob: -16.178442001342773\n",
      "Token: &rsquo, Prob: -18.324764251708984\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  skeleton\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  skeleton, Prob: -0.42230868339538574\n",
      "Token:  scientists, Prob: -1.1908328533172607\n",
      "Token:  eggs, Prob: -4.583273887634277\n",
      "Token:  programmers, Prob: -4.70224666595459\n",
      "Token:  some, Prob: -5.196494102478027\n",
      "\n",
      "\n",
      "\n",
      "Next_Token: s\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token: s, Prob: -8.69767609401606e-06\n",
      "Token:  , Prob: -12.118119239807129\n",
      "Token: ‑, Prob: -13.401862144470215\n",
      "Token:  , Prob: -14.330973625183105\n",
      "Token:  kids, Prob: -15.393021583557129\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  fight\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  fight, Prob: -0.11825496703386307\n",
      "Token:  ever, Prob: -2.2037689685821533\n",
      "Token:  get, Prob: -7.128751277923584\n",
      "Token:  play, Prob: -9.295016288757324\n",
      "Token:  argue, Prob: -9.345248222351074\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  each\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  each, Prob: -1.915549364639446e-05\n",
      "Token:  over, Prob: -11.230415344238281\n",
      "Token:  in, Prob: -12.719686508178711\n",
      "Token: …, Prob: -14.634132385253906\n",
      "Token:  for, Prob: -14.747989654541016\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  other\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  other, Prob: -1.4540854635924916e-06\n",
      "Token:  , Prob: -14.391288757324219\n",
      "Token:  other's, Prob: -14.526552200317383\n",
      "Token: other, Prob: -15.286031723022461\n",
      "Token:  others, Prob: -16.51468849182129\n",
      "\n",
      "\n",
      "\n",
      "Next_Token: ?\n",
      "\n",
      "\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token: ?\n",
      "\n",
      ", Prob: -0.22560183703899384\n",
      "Token: ?, Prob: -1.6014162302017212\n",
      "Token: ?\n",
      "\n",
      "\n",
      "\n",
      ", Prob: -9.223288536071777\n",
      "Token: …, Prob: -9.363308906555176\n",
      "Token:  , Prob: -9.788159370422363\n",
      "\n",
      "\n",
      "\n",
      "Next_Token: They\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token: They, Prob: -0.09789025038480759\n",
      "Token: Because, Prob: -2.3923885822296143\n",
      "Token:  , Prob: -7.407006740570068\n",
      "Token: —, Prob: -7.421260356903076\n",
      "Token: …, Prob: -8.315154075622559\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  don\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  don, Prob: -0.10680286586284637\n",
      "Token:  just, Prob: -2.3602170944213867\n",
      "Token:  have, Prob: -5.326519966125488\n",
      "Token: ’ve, Prob: -7.283677101135254\n",
      "Token:  don't, Prob: -7.966423988342285\n",
      "\n",
      "\n",
      "\n",
      "Next_Token: ’t\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token: ’t, Prob: -1.6354724721168168e-05\n",
      "Token:  , Prob: -11.133307456970215\n",
      "Token: ‑, Prob: -14.077431678771973\n",
      "Token:  , Prob: -14.240872383117676\n",
      "Token: ‘t, Prob: -15.883408546447754\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  have\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  have, Prob: -5.0444748922018334e-05\n",
      "Token:  *, Prob: -10.674721717834473\n",
      "Token:  **, Prob: -11.304120063781738\n",
      "Token:  “, Prob: -12.368279457092285\n",
      "Token: ’ve, Prob: -12.617192268371582\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  the\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  the, Prob: -2.6788466129801236e-05\n",
      "Token:  *, Prob: -11.823360443115234\n",
      "Token:  **, Prob: -12.30351448059082\n",
      "Token:  , Prob: -12.34881591796875\n",
      "Token:  any, Prob: -12.548892974853516\n",
      "\n",
      "\n",
      "\n",
      "Next_Token:  guts\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token:  guts, Prob: -0.014767196029424667\n",
      "Token:  *, Prob: -4.680271625518799\n",
      "Token:  **, Prob: -5.588806629180908\n",
      "Token:  stomach, Prob: -7.320953845977783\n",
      "Token:  heart, Prob: -7.686737537384033\n",
      "\n",
      "\n",
      "\n",
      "Next_Token: .\n",
      "\n",
      "Top probabilities for next token\n",
      "\n",
      "\n",
      "Token: ., Prob: -0.26365724205970764\n",
      "Token: !, Prob: -1.4653593301773071\n",
      "Token:  for, Prob: -7.878309726715088\n",
      "Token: …, Prob: -8.266205787658691\n",
      "Token: —, Prob: -10.284714698791504\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model = OLLAMA_MODEL, \n",
    "    messages = messages, \n",
    "    logprobs = True,\n",
    "    top_logprobs = 5,\n",
    "    temperature = 0,\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(f\"Next_Token: {chunk.choices[0].delta.content}\", end = '\\n\\n')\n",
    "        print(\"Top probabilities for next token\", end = \"\\n\\n\\n\")\n",
    "        log_probs = chunk.choices[0].logprobs.content[0].top_logprobs\n",
    "        for next_token in log_probs:\n",
    "            print(f\"Token: {next_token.token}, Prob: {next_token.logprob}\")\n",
    "        print(end = '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f04f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
