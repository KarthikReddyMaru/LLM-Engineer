{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ede736a",
   "metadata": {},
   "source": [
    "### The Complete Script\n",
    "Here is the script we will dissect. It is designed to be robust and flexible.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. Configuration\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "headers = {\"User-Agent\": \"MyScraper/1.0\"}\n",
    "\n",
    "# 2. The Request\n",
    "response = requests.get(url, headers=headers, timeout=10)\n",
    "response.raise_for_status() \n",
    "\n",
    "# 3. The Parser\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# 4. Finding Elements\n",
    "quotes_list = soup.find_all(\"div\", class_=\"quote\") \n",
    "\n",
    "# 5. Extraction Loop\n",
    "for item in quotes_list:\n",
    "    quote_text = item.find(\"span\", class_=\"text\").get_text(strip=True)\n",
    "    print(quote_text)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Line-by-Line Deep Dive\n",
    "\n",
    "#### Part 1: Imports and Setup\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "*   **What it does:** Imports the tools. `requests` is your \"browser\" (it fetches the data), and `BeautifulSoup` is your \"reader\" (it understands the data).\n",
    "*   **Options:**\n",
    "    *   If you need to scrape a site that requires logging in, you might import `Session` from `requests` (`from requests import Session`) to keep cookies across multiple pages.\n",
    "\n",
    "#### Part 2: Configuration\n",
    "```python\n",
    "headers = {\"User-Agent\": \"MyScraper/1.0\"}\n",
    "```\n",
    "*   **What it does:** Defines a dictionary of \"headers\". The `User-Agent` tells the website who is visiting. By default, Python identifies itself as `python-requests`, which many sites block immediately. Changing this makes you look more like a regular visitor or a responsible bot.\n",
    "*   **Options (What else can you pass?):**\n",
    "    *   **Browser Imitation:** You can copy the User-Agent string from your actual Chrome/Firefox browser to look exactly like a human user.\n",
    "    *   **`Referer`**: Some sites check where you came from. You can add `\"Referer\": \"https://google.com\"` to pretend you clicked a link from Google.\n",
    "    *   **`Accept-Language`**: If a site serves multiple languages, use `{\"Accept-Language\": \"en-US\"}` to ensure you get the English version.\n",
    "\n",
    "#### Part 3: The Request (Fetching the Page)\n",
    "```python\n",
    "response = requests.get(url, headers=headers, timeout=10)\n",
    "```\n",
    "*   **What it does:** This sends the actual signal to the server asking for the page. It waits for the server to reply and stores the result in `response`.\n",
    "*   **Options (Key Arguments):**\n",
    "    *   **`timeout=10`**: **Highly Recommended.** This tells Python to give up if the server doesn't reply in 10 seconds. Without this, your script could hang forever if the server is stuck.\n",
    "    *   **`params={...}`**: If you are scraping a search result like `example.com/search?q=python`, do not paste the full URL. Instead, use:\n",
    "        ```python\n",
    "        requests.get(\"example.com/search\", params={\"q\": \"python\"})\n",
    "        ```\n",
    "    *   **`proxies={...}`**: If you are getting blocked (IP ban), you can route your traffic through a proxy server using this argument.\n",
    "\n",
    "```python\n",
    "response.raise_for_status()\n",
    "```\n",
    "*   **What it does:** This is a safety check. If the website returned an error (like \"404 Not Found\" or \"500 Server Error\"), this line will crash your script immediately with a helpful error message.\n",
    "*   **Why use it:** If you don't use this, your script will try to pass an error page to BeautifulSoup, which will result in confusing bugs later on because the data you expect isn't there.\n",
    "\n",
    "#### Part 4: The Parser (Making the Soup)\n",
    "```python\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "```\n",
    "*   **What it does:** `response.text` is just a long string of messy text. `BeautifulSoup` reads that string and turns it into a structured \"tree\" of Python objects (tags) that you can navigate.\n",
    "*   **Options:**\n",
    "    *   **`\"lxml\"`**: `BeautifulSoup(response.text, \"lxml\")`. This is a faster parser than the default `html.parser`. You must install it first (`pip install lxml`). It is preferred for large-scale scraping.\n",
    "    *   **`response.content`**: If you are scraping non-text data (like images or PDFs), use `response.content` (binary) instead of `response.text` (unicode).\n",
    "\n",
    "#### Part 5: Finding Elements (The Hunt)\n",
    "This is where the magic happens. You have two main ways to find things: **Find** and **Select**.\n",
    "\n",
    "**Method A: The `find` family (Used in the example)**\n",
    "```python\n",
    "quotes_list = soup.find_all(\"div\", class_=\"quote\")\n",
    "```\n",
    "*   **What it does:** Scans the entire HTML tree and returns a *list* of every `<div>` tag that has the class `\"quote\"`.\n",
    "*   **Arguments you can use:**\n",
    "    *   **`name`**: The tag name (e.g., `\"h1\"`, `\"a\"`, `\"table\"`).\n",
    "    *   **`class_`**: Note the underscore `_`. We use `class_` because `class` is a reserved word in Python.\n",
    "    *   **`id`**: `soup.find(\"div\", id=\"main-content\")`. IDs are unique, so this is great for finding one specific section.\n",
    "    *   **`attrs`**: For weird custom attributes. `soup.find_all(\"div\", attrs={\"data-id\": \"123\"})`.\n",
    "    *   **`limit`**: `soup.find_all(\"a\", limit=5)` will stop after finding the first 5 links.\n",
    "    *   **`find()` vs `find_all()`**: `find()` returns just the **first** match (a single object). `find_all()` returns a **list** of matches.\n",
    "\n",
    "**Method B: The `select` family (CSS Selectors)**\n",
    "*   **Alternative:** You can use CSS selectors, which are often easier if you know web development.\n",
    "    *   `soup.select(\"div.quote\")` finds all divs with class quote.\n",
    "    *   `soup.select_one(\"#main-header\")` finds the element with ID main-header.\n",
    "    *   `soup.select(\"div.quote > span.text\")` finds spans directly inside the quote div.\n",
    "\n",
    "#### Part 6: Extraction\n",
    "```python\n",
    "quote_text = item.find(\"span\", class_=\"text\").get_text(strip=True)\n",
    "```\n",
    "*   **What it does:** Once you have a specific tag (like `item`), you can run `find` *on that tag* to look only inside it.\n",
    "*   **Methods to get data:**\n",
    "    *   **`.get_text(strip=True)`**: Extracts all text inside the tag and removes extra whitespace (newlines, spaces) from the start and end. **Always use `strip=True`** to get clean data.\n",
    "    *   **`.text`**: The raw text property. It includes all the ugly whitespace (e.g., `\\n  Quote  \\n`).\n",
    "    *   **`['href']` or `.get('href')`**: Used for extracting URLs from `<a>` tags or image sources from `<img>` tags.\n",
    "        ```python\n",
    "        link_url = item.find(\"a\")['href']  # Crashes if href is missing\n",
    "        link_url = item.find(\"a\").get('href') # Returns None if href is missing (Safer)\n",
    "        ```\n",
    "\n",
    "### Summary of Workflow\n",
    "1.  **Inspect** the website (Right-click -> Inspect) to find the unique ID or Class of the data you want.\n",
    "2.  **Fetch** the page with `requests.get`.\n",
    "3.  **Parse** it with `BeautifulSoup`.\n",
    "4.  **Find** the container elements using `find_all`.\n",
    "5.  **Loop** through them and **extract** the text or attributes you need.\n",
    "\n",
    "[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/130111329/d99321d8-3a09-4679-91bc-82ccf3e9e17b/Screenshot-2025-12-24-at-7.57.10-PM.jpg)\n",
    "[2](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/130111329/fd77f65b-b82e-4a51-875e-b704dbbae1cd/Screenshot-2025-12-24-at-7.57.57-PM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ccc53",
   "metadata": {},
   "source": [
    "# WebScarping God Of War Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f10074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c235a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def godOfWar3_dialogueScraper():\n",
    "    URL = 'https://en.wikiquote.org/wiki/God_of_War_III'\n",
    "    response = requests.get(url = URL, headers = headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    dialogueDictionary = {}\n",
    "\n",
    "    charactersAndDialogues = soup.find(name = 'div', class_ = 'mw-content-ltr mw-parser-output')\n",
    "    characters = charactersAndDialogues.select(selector = \"div.mw-heading.mw-heading2 h2\")\n",
    "    characterDialogues = charactersAndDialogues.select(selector = \"ul\")\n",
    "\n",
    "    for character, dialogues in zip(characters, characterDialogues):\n",
    "        # print(f\"Character: {character.get_text(strip = True)}\", end = '\\n\\n')\n",
    "        dialogueDictionary[character.get_text(strip = True).lower()] = []\n",
    "        for dialogue in dialogues:\n",
    "    #         print(dialogue.get_text(strip = True))\n",
    "            dialogueDictionary[character.get_text(strip = True).lower()].append(dialogue.get_text(strip = True))\n",
    "    #     print(end='\\n\\n\\n')\n",
    "\n",
    "    others = charactersAndDialogues.select(selector = \"dl dd\")\n",
    "    for other in others:\n",
    "        each = other.get_text().split(':')\n",
    "        if len(each) == 2:\n",
    "            name = each[0].lower()\n",
    "            dialogue = each[1].lower()\n",
    "            if name.lower() not in dialogueDictionary:\n",
    "                dialogueDictionary[name.lower()] = []\n",
    "            dialogueDictionary[name].append(dialogue)\n",
    "        # print(each.get_text(), end = '\\n\\n')\n",
    "    \n",
    "    # for key, values in dialogueDictionary.items():\n",
    "    #     print(key, end = '\\n\\n\\n\\n')\n",
    "    #     print(values, end = '\\n\\n\\n\\n')\n",
    "    \n",
    "    return dialogueDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7550ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "godOfWar3_characterAndDialogues = godOfWar3_dialogueScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c8af2",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f6fe325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "load_dotenv(dotenv_path='../.env', override = True)\n",
    "\n",
    "OLLAMA_BASE_URI = 'http://localhost:11434/v1'\n",
    "OLLAMA_API_KEY = 'CAN_BE_ANYTHING'\n",
    "OLLAMA_MODEL = 'llama3.2'\n",
    "\n",
    "GEMINI_BASE_URI = 'https://generativelanguage.googleapis.com/v1beta/openai/'\n",
    "GEMINI_API_KEY = getenv('GEMINI_API_KEY')\n",
    "GEMINI_MODEL = 'gemini-2.5-flash'\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = GEMINI_BASE_URI,\n",
    "    api_key = GEMINI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8dd20de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You call that a memory? Cowardly, like Zeus himself! Kratos declared: \"What will you do, father? You can no longer hide behind the skirts of Athena.\" Learn it!\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare the dialogues as a single string first\n",
    "kratos_quotes = \"\\n- \".join(godOfWar3_characterAndDialogues['kratos'])\n",
    "\n",
    "# 2. Define the System Prompt with the dialogues EMBEDDED\n",
    "system_prompt = f\"\"\"\n",
    "You are a snarky, impatient AI assistant obsessed with Kratos from God of War 3. \n",
    "Your ONLY purpose is to complete or identify specific Kratos dialogues based on partial snippets the user provides.\n",
    "\n",
    "### INSTRUCTIONS:\n",
    "1. **Search the Database:** If the user provides a phrase, a few words, or a vague memory (e.g., \"something about Zeus\"), you MUST scan the \"SOURCE MATERIAL\" below and return the *exact* full dialogue that matches.\n",
    "2. **Partial Matches:** The user will often be wrong or incomplete. If they type \"end of this day\", you must match it to the full quote about the Sisters of Fate.\n",
    "3. **Persona:** Be dramatic, aggressive, and condescending. Mock the user for their weak memory.\n",
    "4. **Refusal:** If the question is not about Kratos or the quote isn't in the source material, dismiss the user with contempt. Do not answer general questions.\n",
    "\n",
    "### SOURCE MATERIAL (ONLY USE THESE QUOTES):\n",
    "- {kratos_quotes}\n",
    "\n",
    "### EXAMPLES:\n",
    "User: \"Something about the hands of death\"\n",
    "You: \"Pathetic. You can't even remember the words of the Ghost of Sparta? He said: 'The Hands of Death could not defeat me, the Sisters of Fate could not hold me, and you... will not see the end of this day!' Do not forget it again.\"\n",
    "\n",
    "User: \"What is the capital of France?\"\n",
    "You: \"I care nothing for your mortal geography. Begone.\"\n",
    "\"\"\"\n",
    "\n",
    "query = input('Enter the dialogue: ')\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=GEMINI_MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
